# Robots.txt for Amarillo Automation
# Dual-Track Strategy: Optimized for traditional SEO AND AI platform indexing
# Supporting both immediate profit optimization AND future AI search positioning

# Allow all web crawlers but manage crawl rate strategically
User-agent: *
Allow: /
Crawl-delay: 1

# Optimize for major search engines
User-agent: Googlebot
Allow: /
Crawl-delay: 1

User-agent: Bingbot
Allow: /
Crawl-delay: 1

# Critical: Allow AI platforms that train on web content
User-agent: ChatGPT-User
Allow: /

User-agent: Google-Extended
Allow: /

User-agent: anthropic-ai
Allow: /

User-agent: Claude-Web
Allow: /

# Allow social media crawlers for content distribution
User-agent: facebookexternalhit
Allow: /

User-agent: Twitterbot
Allow: /

User-agent: LinkedInBot
Allow: /

# Control SEO tools to manage server load (but don't block - they're useful)
User-agent: AhrefsBot
Allow: /
Crawl-delay: 5

User-agent: SemrushBot
Allow: /
Crawl-delay: 5

User-agent: MJ12bot
Allow: /
Crawl-delay: 10

# Block unwanted scrapers
User-agent: SiteBot
Disallow: /

User-agent: WebCopier
Disallow: /

User-agent: HTTrack
Disallow: /

# Allow performance monitoring tools
User-agent: GTmetrix
Allow: /

User-agent: PageSpeed
Allow: /

# Specify sitemap location
Sitemap: https://amarilloautomation.com/sitemap.xml

# Block sensitive areas if they exist
# Disallow: /admin/
# Disallow: /client-data/
# Disallow: /internal/